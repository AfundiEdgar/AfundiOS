

ğŸ§  AfundiOS â€” Your Personal AI Operating System
A full-stack RAG platform that ingests your knowledge, stores it, and answers with context.

AfundiOS is a personal AI memory engine that lets you upload files, scrape websites, ingest YouTube videos, and build your own private knowledge base. It retrieves what youâ€™ve learned, synthesizes it using LLMs, and behaves like an AI assistant that â€œremembers everything you feed it.â€

This project demonstrates production-grade AI engineering, including:

Retrieval-Augmented Generation (RAG)

Vector databases

Embedding pipelines

Agent-style reasoning

Full-stack architecture (FastAPI + Streamlit/Next.js)

Document ingestion + text extraction

Docker containerization

Clean API design
ğŸš€ Features
ğŸ” Intelligent RAG Query Engine

Ask natural language questions and get:

synthesized answers

citations

source documents and highlighted chunks

ğŸ“¥ Multi-Source Ingestion

Upload or link:

PDFs

Markdown

Text files

Entire webpages

YouTube videos (Whisper transcript)

ğŸ§© Modular AI Pipeline

Automatic text extraction

Semantic chunking

Embedding generation

Vector storage (Chroma/FAISS)

Reranking

LLM synthesis

ğŸ“Š Knowledge Dashboard

See:

total documents

total chunks

timeline of ingestion

topic distribution

most queried themes

ğŸ’¬ Real-Time Chat Interface

A clean interface for conversational interaction with your knowledge base.

ğŸ” Private by Default

Local vector stores, optional encryption, no cloud syncing.

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Frontend      â”‚
                â”‚ Streamlit/Next.js â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ REST API
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      FastAPI       â”‚
                â”‚  (Backend Layer)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚               â”‚                â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Extraction     â”‚ â”‚  Embeddings   â”‚ â”‚   LLM Layer  â”‚
 â”‚ (PDF, Web, YT)  â”‚ â”‚ (Chroma/FAISS)â”‚ â”‚ (OpenAI etc.)â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚               â”‚                â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               End-to-End RAG Pipeline
ğŸ“š Tech Stack
Backend

Python

FastAPI

LangChain / LlamaIndex

ChromaDB or FAISS

OpenAI APIs

Whisper for transcription

Frontend

Streamlit (simple)
or

Next.js + Tailwind (polished)

Deployment

Docker

Render / Vercel
ğŸ“¦ Project Structure
afundios/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚   â”œâ”€â”€ query.py
â”‚   â”‚   â”œâ”€â”€ health.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ extractor.py
â”‚   â”‚   â”œâ”€â”€ chunker.py
â”‚   â”‚   â”œâ”€â”€ embedder.py
â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â””â”€â”€ metadata.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ components/
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_folder.py
â”‚   â””â”€â”€ rebuild_index.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_query.py
â”‚   â””â”€â”€ test_api.py
â”‚
â””â”€â”€ docker-compose.yml
âš™ï¸ API Endpoints
POST /ingest

Uploads a file or URL into the knowledge base.

POST /query

Runs full RAG retrieval + LLM synthesis.

GET /stats

Returns DB statistics and memory analytics.
ğŸ”¥ How It Works (RAG Pipeline)
1. Ingestion
File/URL â†’ Extract â†’ Clean â†’ Chunk â†’ Embed â†’ Store

2. Retrieval
Query â†’ Embed â†’ Retrieve top-K â†’ Rerank

3. Synthesis
Chunks â†’ LLM â†’ Structured Answer + Citations

ğŸ§ª Quick Start

### Prerequisites
- Python 3.11+
- pip or conda
- 4GB RAM minimum

### 1. Clone & Setup

```bash
git clone https://github.com/yourname/afundios.git
cd afundios

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
# Copy configuration template
cp .env.example .env

# Edit with your API keys
nano .env  # or use your editor

# Validate configuration
python -m backend.config_validator
```

**Required for basic usage**:
- `OPENAI_API_KEY` (or other LLM provider key)
- `ENVIRONMENT=local`
- `VECTOR_STORE_TYPE=chroma` (default)

### 3. Run Backend

```bash
# Terminal 1: Start backend API
python -m backend.main
# or
uvicorn backend.main:app --reload
```

**Expected output**:
```
âœ“ Configuration valid
âœ“ Vector store initialized
âœ“ Uvicorn running on http://127.0.0.1:8000
```

### 4. Run Frontend

**Option A: Streamlit (Simple & Fast)**

```bash
# Terminal 2: Start Streamlit
streamlit run frontend/app.py
```

Open http://localhost:8501

**Option B: Next.js (Polished UI)**

```bash
# Terminal 2
cd frontend
npm install
npm run dev
```

Open http://localhost:3000

### 5. Test the System

**Via Frontend**:
1. Upload a PDF or text file
2. Ask a question about it
3. See cited sources

**Via API**:
```bash
# Upload a file
curl -X POST http://localhost:8000/ingest \
  -F "file=@document.pdf"

# Ask a question
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is this document about?"}'

# Get statistics
curl http://localhost:8000/stats
```

---

## ğŸ“– Getting Started for Contributors

### For New Developers

1. **Start here**: Read this README for overview
2. **Setup**: Follow Quick Start above
3. **Optional Features**: Read [OPTIONAL_FEATURES_GUIDE.md](./OPTIONAL_FEATURES_GUIDE.md)
   - Switching vector stores (Chroma â†’ FAISS â†’ Pinecone)
   - Using local LLMs (Ollama, LM Studio)
   - Enabling encryption
   - Setting up automated maintenance

4. **Configuration**: Check [CONFIGURATION_MANAGEMENT.md](./CONFIGURATION_MANAGEMENT.md)
   - All available options
   - Validation rules
   - Error messages

5. **Testing**: Run tests
   ```bash
   pytest tests/ -v
   ```

6. **Architecture**: See Project Structure section below

### Common Development Tasks

#### Run Tests
```bash
pytest tests/ -v
pytest tests/test_api.py -k "ingest" -v
pytest tests/ --cov=backend
```

#### Check Configuration
```bash
python -m backend.config_validator
```

#### View API Documentation
```
http://localhost:8000/docs
```

#### Check Code Quality
```bash
# Linting (if installed)
flake8 backend/
pylint backend/

# Type checking (if installed)
mypy backend/
```

#### Add a New Feature

Example: Add support for a new file type

1. **Add extractor** in `backend/core/extractor.py`
   ```python
   def extract_xyz(file_path: str) -> str:
       """Extract text from .xyz files"""
       # Implementation
       return text
   ```

2. **Add to ingest pipeline** in `backend/api/ingest.py`
   ```python
   if file.filename.endswith('.xyz'):
       text = extract_xyz(temp_file.name)
   ```

3. **Add test** in `tests/test_ingestion.py`
   ```python
   def test_ingest_xyz():
       # Test implementation
       assert result.chunks > 0
   ```

4. **Update docs** in README.md

---

## ğŸ› ï¸ Advanced Configuration

Not sure which features to enable? Here are preset configurations:

### ğŸš€ Production Setup
```bash
# See OPTIONAL_FEATURES_GUIDE.md: "Complete Example: Production Setup"
LLM_PROVIDER=local
LOCAL_LLM_URL=http://localhost:11434/api
VECTOR_STORE_TYPE=faiss
ENCRYPTION_ENABLED=true
MEMORY_COMPACTION_ENABLED=true
DAILY_BRIEFING_ENABLED=true
```

### ğŸ“ Development Setup
```bash
# Fast iteration with cloud LLM
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
VECTOR_STORE_TYPE=chroma
ENCRYPTION_ENABLED=false
MEMORY_COMPACTION_ENABLED=false
```

### ğŸ”¬ Experimental Setup
```bash
# Test new features
VECTOR_STORE_TYPE=pinecone
LLM_PROVIDER=anthropic
ENCRYPTION_ENABLED=true
DAILY_BRIEFING_ENABLED=true
```

---

## ğŸ“š Complete Documentation Index

**See [DOCUMENTATION_INDEX.md](./DOCUMENTATION_INDEX.md) for complete reference of all 37 documentation files (15,000+ lines)**

### Essential Reading (Start Here!)

1. **[README.md](./README.md)** â† You are here
2. **[CONTRIBUTOR_QUICK_START.md](./CONTRIBUTOR_QUICK_START.md)** â€” 15-minute setup
3. **[GETTING_HELP.md](./GETTING_HELP.md)** â€” Help & navigation guide

### Core Documentation

| Document | Purpose | Time |
|----------|---------|------|
| [CONFIGURATION_MANAGEMENT.md](./CONFIGURATION_MANAGEMENT.md) | Configure API keys & settings | 20 min |
| [API_REFERENCE.md](./API_REFERENCE.md) | All REST endpoints with examples | 30 min |
| [OPTIONAL_FEATURES_GUIDE.md](./OPTIONAL_FEATURES_GUIDE.md) | Advanced features (vector stores, local LLMs, etc.) | 30-60 min |
| [FEATURE_IMPLEMENTATION_GUIDE.md](./FEATURE_IMPLEMENTATION_GUIDE.md) | How to add new features | 20 min read |
| [TROUBLESHOOTING_GUIDE.md](./TROUBLESHOOTING_GUIDE.md) | Fix 50+ common issues | 5-30 min |
| [DEPLOYMENT_GUIDE.md](./DEPLOYMENT_GUIDE.md) | Deploy to production (AWS, GCP, Docker) | 1-2 hrs |

### Quick Reference by Use Case

| I want to... | Read... | Time |
|-------------|---------|------|
| Get started in 15 min | [CONTRIBUTOR_QUICK_START.md](./CONTRIBUTOR_QUICK_START.md) | 15 min |
| Configure system | [CONFIGURATION_MANAGEMENT.md](./CONFIGURATION_MANAGEMENT.md) | 20 min |
| Find help | [GETTING_HELP.md](./GETTING_HELP.md) | 5 min |
| Understand API | [API_REFERENCE.md](./API_REFERENCE.md) | 30 min |
| Use advanced features | [OPTIONAL_FEATURES_GUIDE.md](./OPTIONAL_FEATURES_GUIDE.md) | 30-60 min |
| Add a feature | [FEATURE_IMPLEMENTATION_GUIDE.md](./FEATURE_IMPLEMENTATION_GUIDE.md) | 20 min |
| Fix a problem | [TROUBLESHOOTING_GUIDE.md](./TROUBLESHOOTING_GUIDE.md) | 5-30 min |
| Deploy to production | [DEPLOYMENT_GUIDE.md](./DEPLOYMENT_GUIDE.md) | 2-4 hrs |
| See all docs | [DOCUMENTATION_INDEX.md](./DOCUMENTATION_INDEX.md) | 10 min |

---

## ğŸ§ª Running Locally (Legacy)

For quick reference, the minimal setup:

```bash
# 1. Install
pip install -r requirements.txt

# 2. Configure
cp .env.example .env
# Edit .env with your API keys

# 3. Run backend
python -m backend.main

# 4. Run frontend
streamlit run frontend/app.py
```

ğŸ“ˆ Roadmap / Stretch Features

ğŸŒ Offline mode using local Llama models

ğŸ” Encrypted vector store

ğŸ§  Graph-based memory visualization

âœ¨ Agents (Planner + Critic + Summarizer)

ğŸŒ“ Automatic long-term memory compression

ğŸ” Scheduled daily briefings
ğŸ‘¤ Author

Michael Edgar Afundi Malongo
AI Engineer â€” Focused on RAG, autonomous agents, and knowledge systems.
backend/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ query.py
â”‚   â”œâ”€â”€ health.py
â”‚   â””â”€â”€ router.py
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ extractor.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ vectorstore.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ llm.py
â”‚   â””â”€â”€ pipeline.py
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ schemas.py
â”‚   â”œâ”€â”€ metadata_store.py
â”‚   â””â”€â”€ vector_store/
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ file_utils.py
â”‚   â”œâ”€â”€ text_utils.py
â”‚   â”œâ”€â”€ yt_utils.py
â”‚   â””â”€â”€ web_utils.py
â”‚
â””â”€â”€ models/
    â”œâ”€â”€ request_models.py
    â”œâ”€â”€ response_models.py
    â””â”€â”€ llm_models.py
